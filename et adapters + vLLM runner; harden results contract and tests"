[1mdiff --git a/iwc/cli.py b/iwc/cli.py[m
[1mindex d1543e4..62d037b 100644[m
[1m--- a/iwc/cli.py[m
[1m+++ b/iwc/cli.py[m
[36m@@ -26,9 +26,12 @@[m [mfrom iwc.compile import ([m
     compile_jsonl_prompts,[m
     compile_sharegpt,[m
 )[m
[32m+[m[32mfrom iwc.compile_alpaca import AlpacaConfig, compile_alpaca[m
 from iwc.export import ExportAiperfConfig, export_aiperf[m
 from iwc.labeler.heuristics import label_record[m
 from iwc.diff.cli import add_diff_subcommand[m
[32m+[m[32mfrom iwc.run_vllm import VllmRunConfig, run_vllm[m
[32m+[m[32mfrom iwc.report_results import build_results_report, format_results_report, report_to_dict[m
 [m
 [m
 # -----------------------------[m
[36m@@ -509,6 +512,28 @@[m [mdef cmd_compile_sharegpt(args: argparse.Namespace) -> None:[m
     print(f"âœ“ Manifest: {manifest_path}")[m
 [m
 [m
[32m+[m[32mdef cmd_compile_alpaca(args: argparse.Namespace) -> None:[m
[32m+[m[32m    out_path = Path(args.output)[m
[32m+[m[32m    manifest_path = Path(args.manifest) if args.manifest else Path(str(out_path) + ".manifest.yaml")[m
[32m+[m
[32m+[m[32m    cfg = AlpacaConfig([m
[32m+[m[32m        max_output_tokens=args.max_output_tokens,[m
[32m+[m[32m        max_output_policy=args.max_output_policy,[m
[32m+[m[32m        temperature=args.temperature,[m
[32m+[m[32m        top_p=args.top_p,[m
[32m+[m[32m        streaming=args.streaming,[m
[32m+[m[32m        prompt_format=args.prompt_format,[m
[32m+[m[32m        arrival=args.arrival,[m
[32m+[m[32m        arrival_step_ms=args.arrival_step_ms,[m
[32m+[m[32m        rate_rps=args.rate_rps,[m
[32m+[m[32m        seed=args.seed,[m
[32m+[m[32m    )[m
[32m+[m
[32m+[m[32m    compile_alpaca(Path(args.input), out_path, manifest_path, cfg)[m
[32m+[m[32m    print(f"âœ“ Workload: {args.output}")[m
[32m+[m[32m    print(f"âœ“ Manifest: {manifest_path}")[m
[32m+[m
[32m+[m
 def cmd_export_aiperf(args: argparse.Namespace) -> None:[m
     manifest_path = Path(args.manifest) if args.manifest else None[m
     cfg = ExportAiperfConfig(time_mode=args.time_mode)[m
[36m@@ -524,6 +549,44 @@[m [mdef cmd_export_aiperf(args: argparse.Namespace) -> None:[m
     print(f"âœ“ Manifest: {mp}")[m
 [m
 [m
[32m+[m[32mdef cmd_run_vllm(args: argparse.Namespace) -> None:[m
[32m+[m[32m    cfg = VllmRunConfig([m
[32m+[m[32m        base_url=args.base_url,[m
[32m+[m[32m        model=args.model,[m
[32m+[m[32m        out_path=Path(args.out),[m
[32m+[m[32m        concurrency=args.concurrency,[m
[32m+[m[32m        timeout_s=args.timeout_s,[m
[32m+[m[32m        max_retries=args.max_retries,[m
[32m+[m[32m    )[m
[32m+[m
[32m+[m[32m    exit_code = run_vllm(Path(args.workload), cfg)[m
[32m+[m
[32m+[m[32m    if exit_code == 0:[m
[32m+[m[32m        print(f"âœ“ All requests succeeded -> {args.out}")[m
[32m+[m[32m    elif exit_code == 2:[m
[32m+[m[32m        print(f"âš  Some requests failed -> {args.out}")[m
[32m+[m[32m    else:[m
[32m+[m[32m        print(f"âœ— All requests failed -> {args.out}")[m
[32m+[m
[32m+[m[32m    raise SystemExit(exit_code)[m
[32m+[m
[32m+[m
[32m+[m[32mdef cmd_report_results(args: argparse.Namespace) -> None:[m
[32m+[m[32m    """Generate aggregate report from runner results."""[m
[32m+[m[32m    report = build_results_report(Path(args.input))[m
[32m+[m
[32m+[m[32m    if args.format == "json":[m
[32m+[m[32m        output = json.dumps(report_to_dict(report), indent=2, sort_keys=True)[m
[32m+[m[32m    else:[m
[32m+[m[32m        output = format_results_report(report)[m
[32m+[m
[32m+[m[32m    if args.output:[m
[32m+[m[32m        Path(args.output).write_text(output + "\n", encoding="utf-8")[m
[32m+[m[32m        print(f"âœ“ Report saved to {args.output}")[m
[32m+[m[32m    else:[m
[32m+[m[32m        print(output)[m
[32m+[m
[32m+[m
 # --------------------[m
 # Main Parser[m
 # --------------------[m
[36m@@ -592,6 +655,44 @@[m [mdef main() -> None:[m
     p_jl.add_argument("--seed", type=int, default=None)[m
     p_jl.set_defaults(func=cmd_compile_jsonl_prompts)[m
 [m
[32m+[m[32m    p_alp = comp_sub.add_parser("alpaca", help="Compile Alpaca-format JSON")[m
[32m+[m[32m    p_alp.add_argument("--input", required=True)[m
[32m+[m[32m    p_alp.add_argument("--output", required=True)[m
[32m+[m[32m    p_alp.add_argument("--manifest", default=None)[m
[32m+[m[32m    p_alp.add_argument("--prompt-format", choices=["raw", "chatml", "openai_messages"], default="raw")[m
[32m+[m[32m    p_alp.add_argument("--max-output-tokens", type=int, default=128)[m
[32m+[m[32m    p_alp.add_argument("--max-output-policy", choices=["fixed", "from_dataset", "clamp"], default="fixed", help="Policy for max_output_tokens (default: fixed)")[m
[32m+[m[32m    p_alp.add_argument("--temperature", type=float, default=0.0)[m
[32m+[m[32m    p_alp.add_argument("--top-p", type=float, default=1.0)[m
[32m+[m[32m    p_alp.add_argument("--streaming", action="store_true")[m
[32m+[m[32m    p_alp.add_argument("--arrival", choices=["fixed-step", "poisson"], default="fixed-step")[m
[32m+[m[32m    p_alp.add_argument("--arrival-step-ms", type=int, default=100)[m
[32m+[m[32m    p_alp.add_argument("--rate-rps", type=float, default=None)[m
[32m+[m[32m    p_alp.add_argument("--seed", type=int, default=None)[m
[32m+[m[32m    p_alp.set_defaults(func=cmd_compile_alpaca)[m
[32m+[m
[32m+[m[32m    # run[m
[32m+[m[32m    p_run = sub.add_parser("run", help="Run a workload against an inference server")[m
[32m+[m[32m    run_sub = p_run.add_subparsers(dest="backend", required=True, title="backends")[m
[32m+[m
[32m+[m[32m    # run vllm[m
[32m+[m[32m    p_run_vllm = run_sub.add_parser("vllm", help="Run against a vLLM server")[m
[32m+[m[32m    p_run_vllm.add_argument("--workload", required=True, help="Path to workload JSONL file")[m
[32m+[m[32m    p_run_vllm.add_argument("--base-url", required=True, help="Base URL of vLLM server (e.g., http://localhost:8000)")[m
[32m+[m[32m    p_run_vllm.add_argument("--model", required=True, help="Model name to use for requests")[m
[32m+[m[32m    p_run_vllm.add_argument("--out", required=True, help="Output JSONL file for results")[m
[32m+[m[32m    p_run_vllm.add_argument("--concurrency", type=int, default=4, help="Maximum concurrent requests (default: 4)")[m
[32m+[m[32m    p_run_vllm.add_argument("--timeout-s", type=float, default=60.0, help="Request timeout in seconds (default: 60.0)")[m
[32m+[m[32m    p_run_vllm.add_argument("--max-retries", type=int, default=2, help="Maximum retries per request (default: 2)")[m
[32m+[m[32m    p_run_vllm.set_defaults(func=cmd_run_vllm)[m
[32m+[m
[32m+[m[32m    # report-results[m
[32m+[m[32m    p_report_res = sub.add_parser("report-results", help="Generate aggregate statistics from runner results")[m
[32m+[m[32m    p_report_res.add_argument("--input", required=True, help="Path to results JSONL file")[m
[32m+[m[32m    p_report_res.add_argument("--output", default=None, help="Output file (default: stdout)")[m
[32m+[m[32m    p_report_res.add_argument("--format", choices=["text", "json"], default="text", help="Output format")[m
[32m+[m[32m    p_report_res.set_defaults(func=cmd_report_results)[m
[32m+[m
     # validate[m
     p_val = sub.add_parser("validate", help="Validate workload JSONL against schema")[m
     p_val.add_argument("paths", nargs="+")[m
